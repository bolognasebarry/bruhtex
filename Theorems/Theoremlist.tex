\documentclass[12pt]{article}
\input{../../../../preamble/preamble.tex}
\input{../../../../preamble/environments.tex}

\usepackage{hyperref}
\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
%This is the start of the prf%
\section{Squeeze Theorem}
\begin{theo}[Squeeze Theorem For Sequences]{}
    Suppose \(\left(a_n\right)^\infty _{n=1}\), \(\left(b_n\right)^\infty _{n=1}\), \(\left(c_n\right)^\infty _{n=1}\) are such that
    \begin{enumerate}
        \item \[a_n \leq b_n \leq c_n \qquad \forall n \in \mathbb{N}\]

        \item \[\Lim_{n\to\infty} a_n = \Lim_{n\to\infty} c_n = L\]
    \end{enumerate}
    Then,
    \[\Lim_{n\to\infty} b_n = L\]
\end{theo}
\begin{prf}{}
    Observe that,  
\begin{align*}
|b_n - L| &= |b_n - a_n + a_n - L|\\
|(b_n - a_n) + (a_n - L)| &\le |b_n - a_n| + |a_n - L| = b_n - a_n + |a_n - L|\\
&\le c_n - a_n + |a_n - L| = |c_n - L + L - a_n| + |a_n - L|\\
&\le |c_n - L| + |L - a_n| + |a_n - L|\\
\end{align*}
Fix $\varepsilon > 0$. There exists $N_1 \in \mathbb N$ such that if $n \ge N_1$ then $$|a_n - L| = |L - a_n| < \frac{\varepsilon}{3}$$.\\
Also, there is $N_2 \in \mathbb N$ such that if $n \ge N_2$ then $$|c_n - L| < \frac{\varepsilon}{3}$$
Now, set $N = \max{N_1, N_2}.$ If $n \ge N$ then, $$|b_n - L| \le |c_n - L| + |L - a_n| + |a_n - L| < \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} = \varepsilon$$ Thus, $\Lim_{n\to\infty}{b_n} = L$. \\
\end{prf}

\newpage
\section{A Convergent Sequence is Bounded}
\begin{theo}[A Convergent Sequence is Bounded]{}
    Convergent sequences are bounded. More precicely, if $(a_n)_{n=1}^\infty$ converges, then there exists $M > 0$ such that $|a_n| \le M$ for all $n \in \mathbb N$\\
\end{theo}
\begin{prf}{}
    Assume $\Lim_{n\to\infty}{a_n} = L$.\\
For every $\varepsilon > 0$, there exists $N \in \mathbb N$ such that if $n \ge N$, then $|a_n - L| < \varepsilon$.\\
This holds for instance if $\varepsilon = 1$.\\
Thus, there exists $N_1 \in \mathbb N$ such that if $n \ge N_1$, then $$|a_n - L| < 1$$\\
By the second triangle inequality, $$|a_n - L| \ge |a_n| - |L|$$\\
Thus, $|a_n| - |L| < 1$ and $|a_n| < 1 + |L|$ \\
Now, $|a_n| \le M = \max\{|a_1|, |a_2|, |a_3|, \cdots, |a_{n-1}|, |L| + 1\}$ for all $n \in \mathbb N$.
\end{prf}

\section{A Bounded, Monotone Sequence Converges}
\begin{theo}[A Bounded, Monotone Sequence Converges]{}
    A monotone sequence converges if and only if it is bounded.
\end{theo}
\begin{prf}{}
    $\left(\implies\right)$\\ This is already proven, see 2\\
$\left(\impliedby\right)$\\ Without loss of generality, assume $(a_n)_{n=1}^\infty$ is increasing (If the sequence is monotone decreasing, a similar argument is used).\\ Define $\alpha = \sup\{a_1, a_2, a_3, \cdots, a_n\} \in \mathbb R$.\\ Since the sequence is bounded, this supremum exists in $\mathbb R$. \\
Let us prove $$\alpha = \Lim_{n\to\infty}{a_n}$$ \\ Chose $\varepsilon > 0$. \\By an earlier theorem, there exists $N \in \mathbb N$ such that $a_N \in (\alpha - \varepsilon, \alpha]$.\\ By monotonicity, if $n\ge N$ then $a_n \ge a_N > \alpha - \varepsilon$. Also, $a_n \le \alpha$. \\ Thus, if $n \ge N$ then $\alpha - \varepsilon < a_n \le \alpha < \alpha + \varepsilon \implies |a_n - a| < \varepsilon$. Thus, $\Lim_{n\to \infty}{a_n} = \alpha$, so $(a_n)_{n=1}^\infty$ converges.\\
\end{prf}
\section{Existence of a Monotone Subsequnce}
\begin{theo}[Existence of a Monotone Subsequnce]{}
    Every sequence of real numbers has a monotone subsequence.
\end{theo}
\begin{prf}{}
We call $k \in \mathbb N$ a peak point of $(a_n)_{n=1}^\infty$ if $a_k > a_n$ for all $n > k$. \\
\underline{Case 1:} There are infinitely many peak points, $n_1 < n_2, n_3 < \cdots$. By definition, $a_{n_1} > a_{n_2} > a_{n_3} >  \cdots$. Thus, $(a_{n_k})_{k=1}^\infty$ is monotone.\\
\underline{Case 2:} There are finitely many peak points. $k_1, k_2, k_3, \cdots, k_l$. Set $$n_1 = \max\{k_1, \cdots, k_l\} + 1$$ Clearly, $n_1$ is not peak. Therefore, there is some $n_2 > n_1$ such that $a_{n_2} \ge a_{n_1}$. Now, $n_2$ is not peak. Therefore there is $n_3 > n_2$ such that $a_{n_3} \ge a_{n_2}$. Continue in this way to obtain a monotone subsequence $(a_{n_i})_{i=1}^\infty$.
\end{prf}

\section{Bolzano-Weiestrass Theorem}
\begin{theo}[Bolzano-Weiestrass Theorem]{}
    Every bounded sequence has a convergent subsequence.
\end{theo}
\begin{prf}{}
    Every sequence has a monotone subsequence. Every subsequence of a bounded sequence is bounded, thus, we have a bounded, monotone subsequence which must converge. 
\end{prf}
\newpage
\section{Limit Arithmetic}
\begin{theo}[Limit Arithmetic]{}
Assume \(\Lim_{x\to a}f(x) = L \quad \Lim_{x\to a}g(x) = m\).\\
Then,
\begin{enumerate}
    \item \(\Lim_{x\to a}(f(x) + g(x)) = L + m\)
    \item \(\Lim_{x\to a}(f(x)\cdot g(x)) = L\cdot m\)
    \item \(\Lim_{x\to a}\frac{f(x)}{g(x)} = \frac{L}{m}\)\\
\end{enumerate}    
\end{theo}
\begin{prf}{}
\begin{enumerate}
    \item Fix \(\varepsilon > 0\). We need to find \(\delta > 0\) such that if \(0 < |x - a| < \delta\) then \(|f(x)+ g(x) - L + m| < \varepsilon\).\\
    \begin{align*}
        |f(x) + g(x) - (L + m)| &= |f(x) + g(x)  - (L + m)|\\
        &= |(f(x) - L) + (g(x) - m)|\\
        &\le |f(x) - L| + |g(x) - m|\\
    \end{align*}
    Since \(\Lim_{x\to a}f(x) = L, \quad \Lim_{x\to a}g(x) = m\), there exists \(\delta_1, \delta_2 > 0\) such that if \(0 < |x - a| < \delta_1, \delta_2\) then, 
    \begin{align*}
        |f(x) - L| &< \frac{\varepsilon}{2}\\
        |g(x) - m| &< \frac{\varepsilon}{2}\\
    \end{align*}
    Choosing \(\delta = \min\{\delta_1, \delta_2\}\), we have that,
    \begin{align*}
    |f(x) - L| + |g(x) - m|  &< \frac{\varepsilon}{2} + \frac{\varepsilon}{2}\\
    &= \varepsilon
    \end{align*}
    \item Fix \(\varepsilon > 0\). We need to find \(\delta > 0\) such that if \(0 < |x - a| < \delta\) then \(|f(x)g(x) - Lm| < \varepsilon\).\\
    \begin{align*}
        |f(x)g(x) - Lm| &= |f(x)g(x) - f(x)m + f(x)m - Lm|\\
        &= |f(x)(g(x) - m) + m(f(x) - L)|\\
        &\le |f(x)||g(x) - m| + |m||f(x) - L|\\
    \end{align*}
    Since \(\Lim_{x\to a}f(x) = L\), there exists \(\delta > 0\) such that if \(0 < |x - a| < \delta\) then, \(|f(x) - L| < 1\).\\
    In this case, \(|f(x)| - |L| < 1 \implies |f(x)| < 1 + |L|\).\\
    Thus, if \(0 < |x - a| < \delta\), then \(|f(x)g(x) - Lm| \le |1 + |L|||g(x) - m| + |m||f(x) - L|\).\\
    There exists \(\delta_2 > 0\) such that if \(0 < |x-a| < \delta_2\) then, \(|g(x) - m| < \frac{\varepsilon}{2(|m| + 1)}\).\\
    Set \(\delta = \min\{\delta_1, \delta_2, \delta_3\}\),\\
    If \(0 < |x-a| < \delta\) then,
    \begin{align*}
        |f(x)g(x) - Lm| &\le (1+|L|)|g(x) - m| + |m||f(x) - L|\\
        &< \frac{\varepsilon}{2(|L| + 1)}\cdot(|L| + 1) + \frac{\varepsilon}{2(|m| + 1)} \cdot 2(|m| + 1)
        &\le \frac{\varepsilon}{2} + \frac{\varepsilon}{2}\\
        &= \varepsilon 
    \end{align*}
    \item We need to show first that \(\Lim_{x\to a}\frac{1}{g(x)} = \frac{1}{m}\)\\
    Fix \(\varepsilon > 0\). \\
    Since \(\Lim_{x\to a}g(x) = m\), there exists \(\delta_1 > 0\) such that if \(0 < |x - a| < \delta_1\) then, \(|g(x) - m| < \frac{|m|}{2}\).\\
    \begin{align*}
        |m| &= |m + g(x) - g(x)|\\
        &\le |g(x) - m| + |g(x)|\\
        &< \frac{|m|}{2} + |g(x)|\\
        \frac{|m|}{2} &< |g(x)|\\
        \frac{1}{g(x)} &< \frac{2}{|m|}
    \end{align*}
    Indeed, there exists some \(\delta_2 > 0\) such that if \(0 < |x - a| < \delta_2\) then \(|g(x) - m| < \frac{|m|^2}{2}\varepsilon\).\\
    Choosing \(\delta = \min\{\delta_1, \delta_2\}\), if \(0 < |x - a| < \delta\) we have,\\
    \begin{align*}
        \bigg\vert\frac{1}{g(x) - \frac{1}{m}}\bigg\vert &= \bigg\vert\frac{m - g(x)}{mg(x)}\bigg\vert\\
        &= \frac{1}{|mg(x)|}|g(x) - m|\\
        &< \frac{1}{|m|}\frac{2}{|m|}|g(x) - L|\\
        &< \frac{2}{|m|^2}\frac{|m|^2}{2}\varepsilon\\
        &= \varepsilon
    \end{align*}
    Now we have proven that \(\Lim_{x\to a} \frac{1}{g(x)} = \frac{1}{m}\), the more general fact follows.\\
    \begin{align*}
        \Lim_{x\to a} \frac{f(x)}{g(x)} &=  \Lim_{x\to a} f(x) \frac{1}{g(x)}\\
        &= \Lim_{x\to a} f(x)\Lim_{x\to a}\frac{1}{g(x)}\\
        &= L\frac{1}{m}\\
        &= \frac{L}{m}
    \end{align*}
\end{enumerate}   
\end{prf}

\section{Differentiability implies Continuity}
\begin{proposition}[Differentiability implies Continuity]{}
    Suppose $f$ is differentiable at $x_0$. Then $f$ is continuous at $x_0$.  
\end{proposition}
\begin{prf}{}
Observe that:\\
\begin{align*}
    \lim_{x\to\ x_0}{f(x)} &= \lim_{x\to x_0}{\left(\frac{f(x) - f(x_0)}{x - x_0}\cdot(x-x_0) + f(x_0)      \right)}\\ 
    &= \lim_{x\to x_0}{\frac{f(x) - f(x_0)}{x - x_0}} \cdot \lim_{x\to x_0}{(x - x_0)} + f(x_0)\\
    &= f'(x_0)\cdot (x_0 - x_0) + f(x_0)\\
    &= 0\cdot f'(x_0) + f(x_0) = f(x_0)
\end{align*}
Therefore, $f$ is continuous at $x_0$.
\end{prf}

\section{Derivative Arithmetic}
\begin{theo}[Derivative Arithmetic]{}
Suppose \(f,g: (a, b) \to \mathbb R\) are both differentiable at \(x \in (a, b)\). Then, \(f + g, fg, \frac{f}{g}\) are differentiable at \(x\). We need \(g(x) \neq 0\) for \(\frac{f}{g}\).\\
\begin{enumerate}
    \item \((f+g)'(x) = f'(x) + g'(x)\)
    \item \((fg)'(x) = f'(x)g(x) + g'(x)f(x)\)
    \item \(\left(\frac{f}{g}\right)'(x) = \frac{f'(x)g(x) - g'(x)f(x)}{g^2(x)}\) \\
\end{enumerate}    
\end{theo}
\newpage
\begin{prf}{}
\begin{enumerate}
    \item 
    \begin{align*}
    (f+g)'(x) &= \Lim_{h\to 0}\frac{f(x+h) + g(x + h) - f(x) - g(x)}{h}\\
    &= \Lim_{h\to 0}\frac{f(x+h) - f(x) + g(x+h) - g(x)}{h}\\
    &= \Lim_{h\to 0}\frac{f(x+h)-f(x)}{h} + \Lim_{h\to 0}\frac{g(x+h) - g(x)}{h}\\
    &= f'(x) + g'(x)   
    \end{align*}
    \item 
    \begin{align*}
    (fg)'(x) &= \Lim_{h\to 0}\frac{f(x+g)g(x+h) - f(x)g(x)}{h}\\
    &= \Lim_{h\to 0}\frac{f(x+h)g(x+h) - f(x+h)g(x) + f(x+h)g(x) - f(x)g(x)}{h}\\
    &= \Lim_{h\to 0}f(x+h)\frac{g(x+h) - g(x)}{h} + g(x)\frac{f(x+h) - f(x)}{h}\\
    &= \Lim_{h\to 0}f(x+h)\Lim_{h\to 0}\frac{g(x+h) - g(x)}{h} + g(x)\Lim_{h\to 0}\frac{f(x+h) - f(x)}{h}\\
    &= f(x)g'(x) + f'(x)g(x)
    \end{align*}
    \item 
    \begin{align*}
    \left(\frac{f'(x)}{g'(x)}\right) &= \Lim_{h\to 0}\frac{\frac{f(x+h)}{g(x+h)} - \frac{f(x)}{g(x)}}{h}\\
    &=  \Lim_{h\to 0}\frac{\frac{f(x+h)}{g(x+h)}\cdot\frac{g(x)}{g(x)} - \frac{f(x)}{g(x)}\cdot\frac{g(x+h)}{g(x+h)}}{h}\\
    &= \Lim_{h\to 0}\frac{g(x)f(x+h) - f(x)(g(x+h))}{hg(x)g(x+h)}\\
    &= \Lim_{h\to 0}\frac{g(x)f(x+h) -f(x)g(x) + f(x)g(x) - f(x)(g(x+h))}{hg(x)g(x+h)}\\
    &= \Lim_{h\to 0}\frac{g(x)(f(x+h) - f(x)) - f(x)(g(x) - g(x+h))}{hg(x)g(x+h)}\\  
    &= \frac{g(x)\Lim_{h\to 0}\frac{f(x+h) - f(x)}{h} - f(x)\Lim_{h\to 0}\frac{g(x+h) - g(x)}{h}}{\Lim_{h\to 0}g(x)g(x+h)}\\
    &= \frac{g(x)f'(x) - f(x)g'(x)}{g^2(x)}
    \end{align*}
\end{enumerate}    
\end{prf}

\section{Rolle's Theorem}
\begin{theo}[Rolle's Theorem]{}
    Suppose $f: [a, b] \rightarrow \mathbb R$ is continuous on $[a, b]$ and differentiable on (a, b). If $f(a) = f(b)$, then there exists $c \in (a, b)$ such that $f'(c) = 0$.\\
\end{theo}
\begin{prf}{}
    If $f(x) = f(a)$ for all $x \in (a, b)$, then the theorem is obvious. \\Assume $f$ is not constant.\\ Without loss of generality, assume there exists $x_0 \in (a, b)$ such that $f(x_0) < f(a)$. \\(If no such $x_0$ exists, then there exists $x_1 \in (a, b)$ such that $f(x_1) < f(a)$. In this case, a similar argument works.)\\
    By the Extreme Value theorem, $f$ has a global maximum on $[a, b]$, call this $c$. Since,\\ $$f(c) \ge f(x_0) > f(a) = f(b)$$ we know $c \neq a, c \neq b$. Thus, $c$ lies in $(a, b)$, and $f'(c) = 0$
\end{prf}

\section{The Mean Value Theorem}
\begin{theo}[The Mean Value Theorem]{}
    Suppose $f: [a, b] \rightarrow \mathbb R$ is continuous on $[a, b]$ and differentiable on $(a, b)$. Then, there exists $c \in (a, b)$ such that $f'(c) = \frac{f(b) - f(a)}{b- a}$.
\end{theo}
\begin{prf}{}
    Consider the function $\phi(x) = f(x) - f(a) - \frac{f(b) - f(a)}{b - a}\cdot (x - a)$.\\ Observe that\\
\begin{align*}
    \phi(a) &= f(a) - f(a) - \frac{f(b) - f(a)}{b - a}\cdot (a - a) = 0\\
    \phi(b) &= f(b) - f(a) - \frac{f(b) - f(a)}{\cancel{b - a}}\cdot \cancel{(b - a)} = 0
\end{align*}
Applying Rolle's theorem to $\phi(x)$, we obtain the existence of $c \in (a, b)$ such that, \\
  $$\phi'(c) = f'(c) - \frac{f(b) - f(a)}{b - a} = 0$$  
\end{prf}

\newpage
\section{A Vanishing Derivative Implies a Constant Function}
\begin{proposition}[Vanishing Derivative Implies a Constant Function]{}
    Suppose $f: [a, b] \implies \mathbb R$ is continuous on $[a, b]$ and differentiable on $(a, b)$. If $f'(x) = 0$ for all $x \in (a, b)$, then $f$ is constant on $[a, b]$\\
\end{proposition}
\begin{prf}{}
    Take $x \in (a, b]$. Applying the Mean Value theorem on $[a, x]$, we conclude that $f(x) - f(a) = f'(c)(x - a)$ for some $c \in (a, x)$. Therefore, as $f'(c) = 0$, \\
\begin{align*}
    f(x) - f(a) &= 0(x - a)\\
    f(x) &= f(a)
\end{align*}
Thus, $f$ is constant in $[a, b]$.
\end{prf}

\section{A Continuous Function on an interval is Uniformly Continuous}
\begin{theo}[A Continuous Function on an interval is Uniformly Continuous]{}
Suppose \(f\) is continuous on a closed, bounded interval \([a, b]\). Then, \(f\) is uniformly continuous on \([a, b]\).    
\end{theo}
\begin{prf}{}
By contradiction.\\
Assume f is not uniformly continuous.\\ There exists \(\varepsilon_0 > 0\) such that for all \(\delta > 0\), there exists \(x,y \in [a, b]\) with \(|x - y| < \delta\) but \(|f(x) - f(y)| \ge \varepsilon_0\).\\
Take \(\delta = 1\). There exists \(x_1, y_1 \in [a, b]\) such that \(|x_1 - y_1| < 1\) but \(|f(x_1) - f(y_1)| \ge \varepsilon_0\).\\
Take \(\delta = \frac{1}{2}\). There exists \(x_2, y_2 \in [a, b]\) such that \(|x_2 - y_2| < \frac{1}{2}\) but \(|f(x_2) - f(y_2)| \ge \varepsilon_0\).\\
Continue in this way for \(\delta = \frac{1}{n}, \quad \forall n \in \mathbb N\).\\
Then, there exists \(x_n, y_n \in [a, b]\) such that \(|x_n - y_n| < \frac{1}{n}\) but \(|f(x_n) - f(y_n)|\ge \varepsilon_0\).\\
We have thus constructed two sequences.\\
\[\left(X_n\right)_{n=1}^\infty\]
\[\left(Y_n\right)_{n=1}^\infty\]
inside \([a, b]\).\\
Since \([a, b]\) is closed and bounded, \(\left(X_n\right)_{n=1}^\infty\) must have some subsequence \(\left(X_{n_k}\right)_{k=1}^\infty\) which converges to some point \(x_0 \in [a, b]\).
\[\Lim_{k\to\infty}\left(X_{n_k}\right) = x_0\]
\underline{Claim:}\\
The subsequence \(\left(Y_{n_k}\right)_{k=1}^\infty\) converges to \(x_0\).\\
\underline{Proof of the claim:}\\
Given \(\varepsilon > 0\), we look for \(N \in \mathbb N\) such that if \(k \ge N\) then, \(|Y_{n_k} - x_0| < \varepsilon\).\\
\begin{align*}
    |Y_{n_k} - x_0| &= |Y_{n_k} + X_{n_k} - X_{n_k} - x_0|\\
    &\le |Y_{n_k} - X_{n_k}| + |X_{n_k} - x_0|\\
    &< \frac{1}{n_k} + |X_{n_k} - x_0|
\end{align*}
Since \(\left(X_n\right)_{n=1}^\infty\) converges to \(x_0\), for k large enough, we have \(|X_{n_k} - x_0| < \frac{\varepsilon}{2}\).\\
Then taking k large enough such that \(\frac{1}{n_k} < \frac{\varepsilon}{2}\), we have, 
\begin{align*}
    |Y_{n_k} - x_0| &< \frac{1}{n_k} + |X_{n_k} - x_0|\\
    &< \frac{\varepsilon}{2} + \frac{\varepsilon}{2}\\
    &= \varepsilon
\end{align*}
Let us prove that this contradicts the continuity of f.\\
Since f is continuous and since \[\Lim_{k\to\infty}X_{n_k} = \Lim_{n\to\infty}Y_{n_k} = x_0\]
we have that, 
\[\Lim_{k\to\infty}f(X_{n_k}) = \Lim_{n\to\infty}f(Y_{n_k}) = f(x_0)\]
Then, for some \(N \in \mathbb N\), if \(k \ge N\) then \(|f(X_{n_k}) - x_0| < \frac{\varepsilon}{4}\) and \(|f(Y_{n_k}) - x_0| < \frac{\varepsilon}{4}\).\\
On the other hand, 
\begin{align*}
    |f(X_{n_k}) - f(Y_{n_k})| &= |f(X_{n_k}) - f(x_0)  + f(x_0) - f(Y_{n_k})|\\
    &\le |f(X_{n_k}) - f(x_0)| + |f(x_0) - f(Y_{n_k})|\\
    &< \frac{\varepsilon}{4} + \frac{\varepsilon}{4}\\
    &= \frac{\varepsilon}{2}
\end{align*}
Also, we know by construction that \[|f(X_{n_k}) - f(Y_{n_k})| \ge \varepsilon_0\]
Thus, we have a contradiction.
\end{prf}

\newpage
\section{Integrability Condition}
\begin{theo}[Integrability Condition]{}
The function \(f:[a, b] \to \mathbb R\) is integrable if and only if for every \(\varepsilon > 0\), there exists a partition \(P\) such that \[U(f, P) - L(f, P) < \varepsilon\]
\end{theo}
\begin{prf}{}
\(\left(\implies\right)\)\\
Assume f is integrable. Fix \(\varepsilon > 0\). There exists \(P_1\) such that \[\integral_a^b f(x)\deriv x = \integral_{\underline{a}}^bf\deriv x < L(f, P_1) + \frac{\varepsilon}{2}\]
Also, there exists \(P_2\) such that 
\[\integral_a^b f(x)\deriv x = \integral_{a}^{\overline{b}}f\deriv x > U(f, P_1) - \frac{\varepsilon}{2}\]
Define \(P = P_1 \cup P_2\). Then,
\begin{align*}
    U(f, P) - L(f, P) &\le U(f, P_2) - L(f, P_1)\\
    &< \integral_a^b f(x) \deriv x + \frac{\varepsilon}{2} - \integral_a^b f(x)\deriv x + \frac{\varepsilon}{2}\\
    &= \varepsilon
\end{align*}
\(\left(\impliedby\right)\)\\
Fix \(\varepsilon > 0\). There exists \(P\) such that,
\[U(f, P) - L(f, P) < \varepsilon\]
Then, \[\integral_a^{\overline{b}}f(x)\deriv x - \integral_{\underline{a}}^bf(x)\deriv x \le U(f, P) - L(f, P) <  \varepsilon\]
This means, \(\integral_a^{\overline{b}}f(x)\deriv x - \integral_{\underline{a}}^bf(x)\deriv x = 0\).\\
Thus, f is integrable.
\end{prf}

\section{Finite Discontinuities implies Integrability}
\begin{theo}[Finite Discontinuities implies Integrability]{}
If \(f:[a, b]\to \mathbb R\) is continuous at all but finitely many points, then \(f\) is integrable.    
\end{theo}
\begin{prf}{}
\underline{Part 1:}\\
Assume f is continuous on \([a, b]\) then f is uniformly continuous on \([a, b]\).\\
Fix \(\varepsilon > 0\). We will find a partition \(P\) such that \(U(f, P) - L(f, P) < \varepsilon\). This will imply integrability.\\
Uniform continuity implies that there exists \(\delta > 0\) such that if \(|x - y| < \delta\) then \(|f(x) - f(y)| < \frac{\varepsilon}{|b-a|}\).\\
Choose \(P\) so that \(|x_i - x_{i-1}| < \delta \) for all \(i\in 1, 2, \cdots , n\).\\
Here \(P = \{x_0, x_1, x_2, \cdots, x_{n-1}\}\). Then, \[U(f, P) - L(f, P) = \displaystyle\sum_{i=1}^n\left(\displaystyle\sup_{[x_{i-1}, x_i]}f(x) - \displaystyle\inf_{[x_{i-1}, x_i]}f(x)\right)\left(x_i - x_{i-1}\right)\]
Since f is continuous, by the extreme value theorem, there exists \(x_i' \in [x_{i-1}, x_{i}]\) such that, \[f(x_i') = \displaystyle\sup_{[x_{i-1}, x_i]}f(x)\]
And there exists some \(x_i'' \in [x_{i-1}, x_{i}]\) such that, \[f(x_i'') = \displaystyle\inf_{[x_{i-1}, x_i]}f(x)\]
Then, 
\begin{align*}
    U(f, P) - L(f, P) &= \displaystyle\sum_{i=1}^n\left(f(x_i') - f(x_i'')\right)\left(x_i - x_{i-1}\right)\\
    &< \displaystyle\sum_{i=1}^n \frac{\varepsilon}{|b - a|} \left(x_i - x_{i-1}\right)\\
    &= \frac{\varepsilon}{|b - a|} \left(x_1 - x_0 + x_2 - x_1 + \cdots + x_n - x_{n-1}\right)\\
    &=  \frac{\varepsilon}{|b - a|}\left(x_n - x_0\right)\\
    &= \frac{\varepsilon}{|b - a|}|b - a|\\
    &= \varepsilon
\end{align*}
Thus, f is integrable.\\
\underline{Part 2:}\\
Assume f has exactly 1 discontinuity, assume it is at \(C \in (a, b)\). The cases where it is at a or b are treated similarly.\\
Fix \(\varepsilon > 0\). We will find a partition \(P\) such that \(U(f, P) - L(f, P) < \varepsilon\).\\
Define \(\delta = \min\{\frac{\varepsilon}{8M}, \frac{b - C}{2}, \frac{C - a}{2}\}\).\\
Where M is such that \(|f(x)| \le M \quad \forall x \in [a, b] \).\\
By our choice of \(\delta, [C-\delta, C+\delta] \subset [a, b]\).\\
By part 1, f is integrable on \([a, C-\delta]\) and \([C+\delta, b]\).\\
Therefore, there exist partitions \(P_1\) of \([a, C-\delta]\) and \(P_2\) of \([C+\delta, b]\) such that,
\begin{align*}
    U(f, P_1) - L(f, P_1) &< \frac{\varepsilon}{4}\\
    U(f, P_2) - L(f, P_2) &< \frac{\varepsilon}{4}
\end{align*}
Define \(P = P_1 \cup P_2\), a partition of \([a, b]\).\\
\begin{align*}
    U(f, P) - L(f, P) &= U(f, P_1) - L(f, P_1) + U(f, P_2) - L(f, P_2) +\\ &\displaystyle\sup_{[C-\delta, C+\delta]}f(x) - \displaystyle\inf_{[C+\delta, C-\delta]}f(x)(C+\delta - (C-\delta))\\
    &< \frac{\varepsilon}{4} + \frac{\varepsilon}{4} + 2M\cdot 2\delta\\
    &\le \frac{\varepsilon}{2} + \frac{4\varepsilon}{8M}\cdot M\\
    &= \frac{\varepsilon}{2} + \frac{\varepsilon}{2}\\
    &= \varepsilon
\end{align*}
\underline{Part 3:}\\
If f has more than 1 discontinuity, apply part 2 enough times.
\end{prf}

\newpage
\section{A Monotone Function is Integrable}
\begin{theo}[A Monotone Function is Integrable]{}
If \(f:[a, b] \to \mathbb R\) is monotone, then \(f\) is integrable.    
\end{theo}
\begin{prf}{}
Assume without loss of generality that \(f\) is increasing.\\
Fix \(\varepsilon > 0\). We will find a partition \(P = \{x_0, x_1, x_2, \cdots, x_n\}\) such that \(U(f, P) - L(f, P) < \varepsilon\).\\
Let \(P\) be the partition that splits \([a, b]\) into n equal parts.\\
Namely, set \(x_k = a+ k\cdot \frac{b-a}{n}\). Then,
\begin{align*}
    U(f, P) - L(f, P) &= \displaystyle\sum_{i=1}^n\left(\left(\displaystyle\sup_{[x_{i-1}, x_i]}f(x) - \displaystyle\sup_{[x_{i-1}, x_i]}f(x)\right)\left(x_{i} - x_{i-1}\right)\right)\\
    &= \displaystyle\sum_{i=1}^n\left(f(x_i)-f(x_{i-1})\right)\left(\frac{b - a}{n}\right)\\
    &= \frac{b - a}{n}\cdot \left(f(x_1) - f(x_0) + f(x_2) - f(x_1) + \cdots + f(x_n) - f(x_{n-1})\right)\\
    &= \frac{b - a}{n}\cdot \left(f(x_n) - f(x_0)\right)\\
    &= \frac{b - a}{n}\cdot \left(f(b) - f(a)\right)\\
\end{align*}    
Chose \(n > \frac{(b - a)(f(b) - f(a))}{\varepsilon}\).\\
Then, \(U(f, P) - L(f, P) < \varepsilon\) and so \(f\) is integrable.
\end{prf}

\section{The Mean Value Theorem For Integrals}
\begin{theo}[The Mean Value Theorem For Integrals]{}
If \(f\) is continuous on \([a, b]\), then, there exists some \(C \in [a, b]\) such that \(\integral_a^b f(x) \deriv x = f(c)(b-a)\).    
\end{theo}
\newpage
\begin{prf}{}
If \(f\) is constant on \([a, b]\), the result is obvious.\\
Assume \(f\) is not constant on \([a, b]\).\\
Denote \(m  = \displaystyle\inf_{[a, b]}f(x), M = \displaystyle\sup_{[a, b]}f(x)\).\\
Since \(f\) is continuous, by the extreme value theorem, there exists points \(x_m \in [a, b]\) and \(x_M \in [a, b]\) such that \(m = f(x_m), M = f(x_M)\).\\
Without loss of generality, assume \(x_m < x_M\). Observe that 
\[m(a-b) \le \integral_a^b f\deriv x \le M(b-a)\]
\[m \le \frac{\integral_a^b f\deriv x}{(b - a)} \le M\]
Apply the intermediate value theorem on \([x_m, x_M]\). Since \(f(x_m) = m\) and \(f(x_M) = M\), there exists some \(C \in [x_m, x_M]\) such that \(f(C) = \frac{\integral_a^b f\deriv x}{(b-a)}\).\\
Then, \[\integral_a^b f\deriv x = f(C)\cdot (b - a)\]
\end{prf}

\section{Continuity of an Antiderivative}
\begin{theo}[Continuity of an Antiderivative]{}
If \(f\) is integrable on \([a, b]\) then the function \(F(x) = \integral_a^b f(x) \deriv x\) is continuous on \([a, b]\).   
\end{theo}
\newpage
\begin{prf}{}
Suppose \(C \in [a, b]\). We will show F is continuous at C. Let M be such that \(|F(x)| \le M \quad \forall x \in [a, b]\).\\
We want to show that \(\Lim_{h\to 0} F(C+h) = F(C)\) or equivalently, \(\Lim_{h\to 0} (F(C+h) - F(C)) = 0\).\\
We begin by showing \(\lim_{h\to 0^+}(F(C+h) - F(C)) = 0\).\\
Indeed, if \(h > 0\) then, \[F(C+h) - F(C) = \integral_a^{C+h} f\deriv x - \integral_a^C f \deriv x = \integral_C^{C+h} f \deriv x\]
Since \(-M \le f \le M\) on \([a, b]\),
\[\integral_C^{C+h} f\deriv x \le M(\cancel{C}+ h - \cancel{C}) = Mh\]
Also, \(-Mh \le \integral_C^{C+h}f\deriv x\).\\
Therefore, \[|f(C+h)-f(C)| = \bigg\vert\integral_C^{C+h} f\deriv x\bigg\vert \le Mh\]
This means \(\lim_{h\to 0^+}(F(C+h) - F(C)) = 0\) and similarly, \(\lim_{h\to 0^-}(F(C+h) - F(C)) = 0\).\\
\end{prf}

\section{The Fundamental Theorem of Calculus}
\begin{theo}[The Fundamental Theorem of Calculus]{}
Assume \(f: [a, b]\to \mathbb R\) is continuous.\\
Define \(F(x) = \integral_a^x f(t) \deriv t\). Then, F is differentiable on \((a, b), F'(x) = f(x)\).\\
Indeed, \(\integral_a^b f \deriv x = F(b) - F(a)\).    
\end{theo}
\begin{prf}{}
Let us compute \(F'(x)\) for \(x\in(a, b)\). We want to find \(\Lim_{h\to 0} \frac{F(x+h) - F(x)}{h}\).\\
We begin by finding \(\Lim_{h\to 0^+} \frac{F(x+h) - F(x)}{h}\).\\
Given \(h\in (0, b-x)\), we compute, \[F(x+h) - F(x) = \integral_x^{x+h}f(t)\deriv x\]
By the mean value theorem for integrals,
\[\integral_x^{x+h} f(t) \deriv t = f(c)(\cancel{x}+ h - \cancel{x}) = f(c)(h)\]
Where \(c\in [x, x+h]\).\\
Note that \(f\) is uniformly continuous on \([a, b]\). Therefore, given \(\varepsilon > 0\), there exists \(\delta > 0\) such that if \(|x-y| < \delta\) then \(|f(x) - f(y)| < \varepsilon\).\\
If \(h < \delta\) then \(|c- x| < \delta\). Therefore, \[\bigg\vert\frac{F(x+h) - F(x)}{h} - f(x)\bigg\vert = \bigg\vert\frac{f(c)h}{h}-f(x)\bigg\vert = |f(c) - f(x)| < \varepsilon\]
Thus, \(\Lim_{h\to 0^+} \frac{F(x+h) - F(x)}{h} = f(x)\).\\
Similarly, \(\Lim_{h\to 0^-} \frac{F(x+h) - F(x)}{h} = f(x)\).\\
So, \(F'(x) = f(x)\).\\
The Newton-Leibniz formula follows immediately.
\end{prf}

\section{Substitution Formula}
\begin{theo}[Substitution Formula]{}
If \(f, g\) are continuous on \([a, b]\) and \(g\) is differentiable on \((a, b)\) with \(g'\) continuous, then,\\
\[\integral_{g(a)}^{g(b)} f(u) \deriv u = \integral_a^b f(g(x))g'(x)\deriv x\]    
\end{theo}
\begin{prf}{}
Suppose F is an antiderivative of \(f\).\\
By the fundamental theorem of calculus, the left hand side of the equation is equal to, 
\[F(g(b)) - f(g(a))\]
Next, by the chain rule, we have, \[(F\circ g)' = (F'\circ g)\cdot g' = (f\circ g)g'\]
This means \(F\circ g\) is an anti-derivative of \((f\circ g)g'\).\\
Therefore, by the fundamental theorem of calculus, 
\begin{align*}
\integral_a^b (f\circ g)g'\deriv x &= (F\circ g)(b) - (F\circ g)(a)\\
&= F(g(b)) - F(g(a))
\end{align*}
Thus, \(\integral_{g(a)}^{g(b)} f(u) \deriv u = \integral_a^b f(g(x))g'(x)\deriv x\) holds.
\end{prf}

\section{Integration by Parts}
\begin{theo}[Integration by Parts]{}
If \(u, v: [a, b] \to \mathbb R\) are continuous on \([a, b]\) and differentiable on \((a, b)\),
\begin{align*}
    \integral uv' \deriv x &= uv - \integral u'v \deriv x\\
    \integral_a^b uv' \deriv x &= uv\rvert_a^b - \integral_a^b u'v \deriv x 
\end{align*}    
Where \(uv\rvert_a^b = u(b)v(b) - u(a)v(a)\).\\
\end{theo}
\begin{prf}{}
We know that \(uv' = u'v + v'u\).\\
Integrating both sides, we obtain,
\begin{align*}
    \integral (uv)' \deriv x &= \integral u'v\deriv x + \integral v'u \deriv x\\
    uv &= \integral u'v\deriv x + \integral uv' \deriv x\\
    \integral u'v \deriv x &= uv - \integral v'u \deriv x
\end{align*}    
The formula for the definite integral follows from the fundamental theorem of calculus.\\
\end{prf}

\section{Integral Test}
\begin{theo}[Integral Test]{}
Assume \(f\) is a non negative, non increasing continuous function on the interval \([1, \infty)\).\\
Then, \(\displaystyle\int_{1}^\infty f \deriv x\) and \(\displaystyle\sum_{n=1}^\infty f(n)\) converge or diverge together.\\     
\end{theo}
\newpage
\begin{prf}{}
Consider the interval \([1, n+1]\). The set \(P = \{1, 2, 3, 4, \cdots , n+1\}\) is a partition of \([1, n+1]\). Since \(f\) is non increasing,
\[\displaystyle\inf_{[x_{i-1}, x_i]} f = f(x_i), \sup_{[x_{i-1},x_i]} f = f(x_{i-1})\]
Consequently,
\begin{align*}
    U(f,P) &= \displaystyle\sum_{i=1}^n \sup_{[x_{i-1}, x_i]} f(x_i - x_{i-1})\\
    &= \displaystyle\sum_{i=1}^n f(x_{i-1})(i+1-i)\\
    &= \displaystyle\sum_{i=1}^n f(x_{i-1})\\
    &= \displaystyle\sum_{i=1}^n f(i)\\
\end{align*}
Also, \(L(f,P) = \displaystyle\sum_{i=1}^n f(i+1)\).\\
If \(\displaystyle\int_1^\infty f \deriv x\) converges, then, \(L(f, P) = \displaystyle\sum_{i=1}^n f(i+1) = \displaystyle\sum_{i=2}^{n+1}f(i)\) converges as \(n\to\infty\).\\
Thus, \(\displaystyle\sum_{i=1}^\infty f(i)\) converges.\\
If \(\displaystyle\int_1^\infty f \deriv x\) diverges then \(U(f,P) = \displaystyle\sum_{i=1}^n f(i)\) diverges as \(n\to\infty\)\\
Which means, \(\displaystyle\sum_{i=1}^\infty f(i)\) diverges.  
\end{prf}

\section{Limit Comparison Test}
\begin{theo}[Limit Comparison Test]{}
Suppose \(a_n, b_n > 0\) for all \(n \in \mathbb N\).\\
If \(\Lim_{n\to\infty}\frac{a_n}{b_n} = c > 0\) (not \(\infty\))\\
then, \(\displaystyle\sum_{n=1}^\infty a_n\) and \(\displaystyle\sum_{n=1}^\infty b_n\) converge and diverge simultaneously.    
\end{theo}
\begin{prf}{}
Assume \(\displaystyle\sum_{n=1}^\infty b_n\) converges. Since \(\Lim_{n\to\infty}\frac{a_n}{b_n} = c\), there exists some \(N \in \mathbb N\) such that if \(n \ge N\)\(\frac{a_n}{b_n} < 2c\) and \(a_n < 2cb_n\).\\
Now, \(\displaystyle\sum_{n=1}^\infty 2cb_n = 2c\displaystyle\sum_{n=1}^\infty b_n\) converges.\\
By comparison, since \(\displaystyle\sum_{n=N}^\infty a_n \le 2c\displaystyle\sum_{n=N}^\infty b_n \le 2c\displaystyle\sum_{n=1}^\infty b_n\), the series \(\displaystyle\sum_{n=N}^\infty a_n\). Hence, \(\displaystyle\sum_{n=1}^\infty a_n\) converges.\\
Next, suppose \(\displaystyle\sum_{n=1}^\infty a_n\) converges. Then, \(\Lim_{n\to\infty}\frac{b_n}{c_n} = \frac{1}{c} > 0\) and \(\displaystyle\sum_{n=1}^\infty b_n\) converges by the previous argument.
\end{prf}

\section{Ratio Test}
\begin{theo}[Ratio Test]{}
Assume \(a_n > 0\) for all \(n \in \mathbb N\).\\
Then, 
\begin{enumerate}
    \item If \(\displaystyle\limsup_{n\to\infty}\frac{a_{n+1}}{a_n} = r < 1\)\\
Then, \(\displaystyle\sum_{n=1}^\infty a_n\) converges.
    \item If \(\displaystyle\liminf_{n\to\infty} \frac{a_{n+1}}{a_n} > 1\)\\
Then, \(\displaystyle\sum_{n=1}^\infty a_n\) diverges.
\end{enumerate}
\end{theo}
\begin{prf}{}
We assume that \(\Lim_{n\to\infty} \frac{a_{n+1}}{a_n} = r \in \mathbb R\).\\
\begin{enumerate}
    \item Fixing \(S\) such that \(r < S < 1\).\\
    Because \(\Lim_{n\to\infty} \frac{a_{n+1}}{a_n} = r < S\), there exists some \(N \in \mathbb N\) such that if \(n \ge N\) then \(\frac{a_{n+1}}{a_n} < S\).\\
    Then, \(\frac{a_{N+1}}{a_N} < S\) and \(a_{N+1} < S\cdot a_N\).\\
    Also, \(\frac{a_{N+2}}{a_{N+1}} < S\) then \(a_{N+2} < S\cdot a_{N+1} < S^2 a_{N}\).\\
    Next, \(\frac{a_{N+3}}{a_{N+2}} < S\) then \(a_{N+3} < S\cdot a_{N+2} < S^3 a_{N}\).\\
    Continuing like this, we find \(a_{N+k} < S^k \cdot a_N\). Thus, \[\displaystyle\sum_{k=N}^\infty a_n \le \displaystyle\sum_{k=0}^\infty S^k\cdot a_N = a_N\displaystyle\sum_{k=0}^\infty S^k = a_N\frac{1}{1-S}\]
    Thus, \(\displaystyle\sum_{k=N}^\infty a_N\) converges. Hence, \(\displaystyle\sum_{n=0}^\infty a_n\) converges.
    \item Assuming \(\Lim_{n\to\infty}\frac{a_{n+1}}{a_{n}} = r > 1\) then there exists some \(N \in \mathbb N\) such that if \(n \ge N\) then \(\frac{a_n+1}{a_n} > 1\) then \(\frac{a_{N+1}}{a_N} > 1\) so, \(a_{N+1} > a_N\). Also, \(\frac{a_{N+2}}{a_N+1} > 1\) so \(a_{N+2} > a_{N+1} > a_{N}\) and so on.\\
    Thus, \(a_{N+k} > a_{N}\) for all \(k \in \mathbb N\) which means \(a_n\) cannot go to \(0\) as \(n\to\infty\).
\end{enumerate}  
\end{prf}

\section{The Leibniz Test}
\begin{theo}[The Leibniz Test]{}
Assume \(\left(a_n\right)_{n=1}^\infty\) is a sequence such that \(a_n \ge 0\), \(a_n \ge a_{n+1}\) for all \(n\in \mathbb N\) and \(\Lim_{n\to\infty} a_n = 0\),\\
Then, the series \(\displaystyle\sum_{n=1}^\infty(-1)^{n+1}a_n\) converges.\\   
\end{theo}
\begin{prf}{}
Let \(S_n  = \displaystyle\sum_{k=1}^n(-1)^{k+1}a_k\).\\
Observe that \(S_1 \ge S_3 \ge S_5 \ge \cdots\)\\
Indeed, 
\begin{align*}
    S_{2n+3} &= S_{2n+1} - a_{2n + 2} + a_{2n+3}\\
    &= S_{2n+1} + \left(a_2n+3 - a_2n+2\right)\\
    &\le S_{2n+1}
\end{align*}  
Also, \[S_{2n+2} = S_{2n} + \left(a_{2n+1} - a_{2n+2}\right) \ge S_{2n} \]
And, \[S_2 \le S_4 \le S_6 \le \cdots\]
Moreover, \(S_{2n} \le S_{2n+1}\) since \(S_{2n+1} = S_{2n} + a_{2n+1} \ge S_{2n}\).\\
We conclude \(S_{2n} \le S_{2n+1} \le S_1\) and \(S-{2n+1} \ge S_{2n} \ge S_{2}\).\\
Thus, the sequences \(\left(S_{2n}\right)_{n=1}^\infty\) and \(\left(S_{2n+1}\right)_{n=1}^\infty\) are bounded.\\
Hence, they must converge. Thus, we can say, 
\[\Lim_{n\to\infty}S_{2n} = \alpha, \quad \Lim_{n\to\infty}S_{2n+1} = \beta\]
Now, 
\begin{align*}
\beta -\alpha &= \Lim_{n\to\infty}S_{2n+1} - \Lim_{n\to\infty}S_{2n}\\
&= \Lim_{n\to\infty}\left(S_{2n+1} - S_{2n}\right)\\
&= \Lim_{n\to\infty}a_{2n+1}\\
&= 0
\end{align*}
Thus, \(\beta = \alpha\) and \(\Lim_{n\to\infty}S_n = \alpha = \beta\) and \(\displaystyle\sum_{n=1}^\infty(-1)^{n+1}a_n\) converges.
\end{prf}

\section{An Absolutely Convergent Series implies convergence}
\begin{theo}[An Absolutely Convergent Series implies convergence]{}
If \(\displaystyle\sum_{n=1}^\infty a_n\) converges absolutely, then it converges.   
\end{theo}
\begin{prf}{}
Use the cauchy criterion.\\
Fixing \(\varepsilon > 0\). We must show that there exists some \(N \in \mathbb N\) such that if \(p > q \ge N\) then \[\left|\displaystyle\sum_{n = q}^p a_n\right| < \varepsilon\]\\
Since \(\displaystyle\sum_{n=1}^\infty |a_n|\) converges, there exists some \(N \in \mathbb N\) such that \(\left|\displaystyle\sum_{n = q}^p |a_n|\right| < \varepsilon\)\\
By the triangle inequality, \[\left|\displaystyle\sum_{n = q}^p a_n\right| \le \displaystyle\sum_{n=1}^\infty |a_n| = \left|\displaystyle\sum_{n = q}^p |a_n|\right| < \varepsilon \]
\end{prf}

\newpage
\section{Maclaurin Series Examples}
\begin{theo}[Maclaurin Series Examples]{}
The formulas and derivations for the functions, 
\begin{enumerate}
    \item \(f(x) = e^x\)
    \item \(f(x) = \sin(x)\)
    \item \(f(x) = \cos(x)\)
    \item \(f(x) = \frac{1}{1-x}\)
\end{enumerate}    
\end{theo}
\begin{prf}{}
    \begin{enumerate}
        \item Let \(f(x) = e^x\).\\
        Let us write down the Maclaurin Series.\\
        Clearly, \(f^{(n)}(0) = \frac{\deriv^n}{\deriv x^n}e^x\bigg\vert_{x=0} = e^0 = 1\)\\
        Therefore, the series is \(\displaystyle\sum_{n=0}^\infty \frac{x^n}{n!}\).\\
        This converges absolutely for all \(x\in \mathbb R\) by the ratio test.
        \item Let \(f(x) = \sin(x)\).\\
        \begin{align*}
            f(x) &= \sin(x)\\
            f'(x) &= \cos(x)\\
            f''(x) &= - \sin(x)\\
            f'''(x) &= - \cos(x)\\
            f''''(x) &= \sin(x)
        \end{align*}
        \[f^{4k} (x) = \sin(x) \qquad f^{4k+1}(x) = \cos(x)\]
        For all \(k \in \mathbb{N}\)
        \begin{align*}
            \displaystyle\sum^\infty_{n=0} \frac{f^{(n)}(0)}{n!} \cdot x^n &= \frac{\sin(0)}{0!} \cdot x^0 + \frac{\cos(0)}{1!} \cdot x^1 + \frac{- \sin(0)}{2!} \cdot x^2 + \frac{- \cos(0)}{3!}\cdot x^3 + \dots\\
            &= x - \frac{1}{3!} \cdot x^3 + \frac{1}{5!} \cdot x^5\\
            &= \displaystyle\sum^\infty_{n=0} (-1)^n \cdot \frac{x^{2n+1}}{(2n+1)!}
        \end{align*}
        \underline{Note:} the series converges absolutely by the ratio test for all \(x \in \mathbb{R}\)
        \[\limsup_{n \to \infty} \frac{\left(-1\right)^{n+1}\cdot \left(\frac{x^{2n+2}}{\left(2n+2\right)!}\right)}{\left(-1\right)^n\cdot \left(\frac{x^{2n+1}}{\left(2n+1\right)!}\right)} = \limsup_{n \to \infty} \frac{-x}{2n+2} = 0 < 1\]       
        \item Let \(f(x) = \cos(x)\).\\
        \begin{align*}
            f(x) &= \cos(x)\\
            f'(x) &= - \sin(x)\\
            f''(x) &= - \cos(x)\\
            f'''(x) &= \sin(x)\\
            f''''(x) &= \cos(x)
        \end{align*}
        \begin{align*}
            \displaystyle\sum^\infty_{n=0} \frac{f^{(n)}(0)}{n!} \cdot x^n &= \frac{\cos(0)}{0!} \cdot x^0 + \frac{- \sin(0)}{1!} \cdot x^1 + \frac{- \cos(0)}{2!} \cdot x^2 + \frac{\sin(0)}{3!}\cdot x^3 + \dots\\
            &= 1 - \frac{1}{2!} \cdot x^2 + \frac{1}{4!} \cdot x^4 - \frac{1}{6!} \cdot x^6 + \dots\\
            &= \displaystyle\sum^\infty_{n=0} (-1)^{n} \cdot \frac{x^{2n}}{2n!}
        \end{align*}
        For all \(x \in \mathbb{R}\)
        \begin{align*}
            f(x) &= \frac{1}{1-x}\\
            f'(x) &= \frac{1}{(1-x)^2}\\
            f''(x) &= \frac{2}{(1-x)^3}\\
            f'''(x) &= \frac{6}{(1-x)^4}\\
            f''''(x) &= \frac{24}{(1-x)^5}
        \end{align*}
        \begin{align*}
            \displaystyle\sum^\infty_{n=0} \frac{f^{(n)}(0)}{n!} \cdot x^n &= \frac{1}{(1-0)(0!)} \cdot x^0 + \frac{1}{(1-0)^2(1!)} \cdot x^1 + \frac{2}{(1-0)^3(2!)} \cdot x^2\\
            &+ \frac{6}{(1-0)^4(3!)} \cdot x^3 + \frac{24}{(1-0)^5(4!)} \cdot x^4\\
            &= \displaystyle\sum^\infty_{n=0} \frac{n!}{n!} \cdot x^n\\
            &= \displaystyle\sum^\infty_{n=0} x^n
        \end{align*}
        This is a geometric series. It converges if \(x \in (-1,1)\) and diverges otherwise.
    \end{enumerate} 
\end{prf}

\section{Inhomogenous Linear System Solutions}
\begin{proposition}[Inhomogenous Linear System Solutions]{}
If \(p\) is a vector such that \(Ap = b\), then,
\[\{x\in \mathbb R^n | Ax = b\} = \{y + p | y\in \rm{NS}(A)\}\]
\end{proposition}
\begin{prf}{}
Observe that \(A(x+y) = Ax + Ay\) and \(A(\lambda x) = \lambda Ax\) for all \(x, y \in \mathbb R^n\), \(\lambda \in \mathbb R\).\\
Assume \(y \in \rm{NS}(A)\). Let us prove that \(A(y + p) = b\).\\
Indeed, \(A(y+p) = Ay + Ap = 0 + b = b\).\\
Thus,\[\{x\in \mathbb R^n | Ax = b\} \supset \{y + p | y\in \rm{NS}(A)\}\]
Now, assume \(Ax = b\). Clearly, \(x = p + (x - p)\). \\
The vector \(y = x - p\) is in \(\rm{NS}(A)\) because \[A(x - p) = Ax - Ap = b - b = 0\]
Thus, \[\{x\in \mathbb R^n | Ax = b\} \subset \{y + p | y\in \rm{NS}(A)\}\]
\end{prf}

\section{Invertibility of a Matrix and the Null Space}
\begin{theo}[Invertibility of a Matrix and the Null Space]{}
The matrix A is invertible if and only if \(\rm{NS}(A) = \{0\}\).
\end{theo}
\begin{prf}{}
\(\left(\implies\right)\)\\
Assume A is invertible. We know \(0 \in \rm{NS}(A)\). We want to prove that if \(Ax = 0, x = 0\) then,
\begin{align*}
x &= (A^{-1}A)x\\
&= A^{-1}(Ax) = A^{-1}0 = 0
\end{align*}
\(\left(\impliedby\right)\)\\
We claim that if \(\rm{NS}(A) = 0\) then A has a right inverse.\\
Proof of the claim\\
If \(\rm{NS}(A = 0\) then the system \(Ax = 0\) has a unique solution.\\
By an earlier proposition, the system \(Ax = b\) has a unique solution for every \(b \in \mathbb R^n\).\\
Take,
\[
b^1 = \begin{pmatrix}
    1 \\ 0 \\ \vdots\\ 0
\end{pmatrix},
\qquad b^2 = \begin{pmatrix}
    0 \\ 1 \\0\\ \vdots\\ 0
\end{pmatrix},
\qquad \cdots 
\]
Solving \(Ax = b^i\) for \(i = 1, 2, \cdots, n\), we obtain an array of vectors,
\[x^i = 
\begin{pmatrix}
    x_{1i} \\ x_{2i}\\ \vdots\\ x_{ni}
\end{pmatrix}
\]
We construct a matrix, 
\[C = \begin{pmatrix}
    x_{11} & \cdots &x_{1n}\\
    \vdots & \ddots & \vdots\\
    x_{n1} & \cdots & x_{nn}
\end{pmatrix}\]
Clearly, \(AC = I_n\).\\
Thus, C is the right inverse of A.\\
This proves the claim.\\
Now, let us show that A has a left inverse. Note that C has a left inverse, namely, A.\\
By the same argument as in \(\left(\implies\right)\).\\
This implies \(\rm{NS}(C) = \{0\}\). By the claim, C has a right inverse.\\
This inverse is equal to A (proven earlier).\\
Thus, \(AC = CA = I_n\).\\
Thus, C is the inverse of A.
\end{prf}

\section{Linearity of the Determinant}
\begin{theo}[Linearity of the Determinant]{}
Suppose \(u, v, a_1, \cdots, a_n\) are vectors in \(\mathbb R^n\). Consider the matrices,
\[
A = \begin{pmatrix}
    a_1\\ a_2 \\ \vdots \\ a_{r-1} \\ u + \lambda v \\ a_{r+1} \\ \vdots \\ a_n
\end{pmatrix}  
\qquad
B = \begin{pmatrix}
    a_1\\ a_2 \\ \vdots \\ a_{r-1} \\ u \\ a_{r+1} \\ \vdots \\ a_n
\end{pmatrix} 
\qquad
C = \begin{pmatrix}
    a_1\\ a_2 \\ \vdots \\ a_{r-1} \\ v \\ a_{r+1} \\ \vdots \\ a_n
\end{pmatrix} 
\]
Then, \(\det A = \det B + \lambda \det C\)
\end{theo}
\begin{prf}{}
We will argue with induction.\\
The result is obvious if \(n = 1\). Then, \(A = (u + \lambda v), B = u,  C = v\) for some \(u, v \in \mathbb R\).\\
Then, \[\det A = u + \lambda v = \det B + \lambda \det C\]
Assume the result holds for \((n-1)\times (n-1)\) matrices. Let us prove the result for \(A, B, C\) being \(n\times n\) matrices.\\
\underline{Case 1:}\\
Assume \(r = 1\).\\
In this case, 
\begin{align*}
\det A &= \displaystyle\sum_{j = 1}^n (-1)^{1+j} (u_j + \lambda v_j)\det\tilde{A_{1j}}\\
&= \displaystyle\sum_{j=1}^n (-1)^{1+j}u_j \det \tilde{A}_{1j} + \lambda\displaystyle\sum_{j=1}^n(-1)^{1+j}v_j \det \tilde{A_{1j}}\\
\end{align*}
Where \(u = (u_1, u_2, \cdots u_n)\qquad v = (v_1, v_2, \cdots, v_n)\).\\
Now, since \(r = 1\), \(\tilde{A_{1j}} = \tilde{B_{1j}} = \tilde{C_{1j}}\).\\
Therefore, 
\begin{align*}
\det A &= \displaystyle\sum_{j = 1}^n (-1)^{1+j} u_j \det \tilde{B_{1j}} + \lambda\displaystyle\sum_{j = 1}^n (-1)^{1+j} v_j \det \tilde{C_{1j}}\\
&= \det B + \lambda\det C
\end{align*}
\underline{Case 2:}\\
Assume \(r > 1\).\\
In this case, the first rows of \(A, B\) and \(C\) are the same. In fact, they are \(a_1\).\\
Now, \[\det A = \displaystyle\sum_{j=1}^n (-1)^{1+j}A_{1j}\det\tilde{A_{1j}}\]
The matrix \(\tilde{A_{ij}}\) is \((n-1)\times (n-1)\). By the induction hypothesis, \[\det \tilde{A_{1j}} = \det \tilde{B_{1j}} + \lambda\det\tilde{C_{1j}}\]
Therefore,
\begin{align*}
    \det A &= \displaystyle\sum_{j=1}^n (-1)^{j+1} A_{1j}\det\left(\tilde{B_{1j}}+ \tilde{C_{1j}}\right)\\
    &= \displaystyle\sum_{j=1}^n (-1)^{j+1} A_{1j}\det\tilde{B_{1j}} + \lambda\displaystyle\sum_{j=1}^n (-1)^{j+1} A_{1j}\det\tilde{C_{1j}}\\
    &= \displaystyle\sum_{j=1}^n (-1)^{j+1} B_{1j}\det\tilde{B_{1j}} + \lambda\displaystyle\sum_{j=1}^n (-1)^{j+1} C_{1j}\det\tilde{C_{1j}}\\
    &= \det B + \lambda \det C
\end{align*}
\end{prf}

\section{Invertibility and the Determinant}
\begin{theo}[Invertibility and the Determinant]{}
    A matrix A is invertible if and only if \(\det A \neq 0\)
\end{theo}
\begin{prf}{}
Suppose A is some square matrix.
\(\left(\implies\right)\)\\
Since \(AA^{-1} = I_n\), taking the determinant of this,\\
\[\det A \det A^{-1} = 1\]
This clearly shows \(\det A \neq 0\).\\
\(\left(\impliedby \right)\)\\
Cosider the matrix \[G = \frac1{\det A}\left(\left(C_{ij}\right)_{i,j = 1}^n\right)^T\]
Where \(C_{ij} = (-1)^{i+j} \det \tilde{A_ij}\).\\
We claim that \[GA = AG = I_n\]
Indeed, given \(k = 1, \cdots, n\), we find,
\begin{align*}
    \left(AG\right)_{KK} &= \displaystyle\sum_{i=1}^n A_{ki} G_{ki} \\
    &= \displaystyle\sum_{i=1}^n A_{ki}(-1)^{i+k} \det \tilde{A_{ki}}\\
    &= \frac1{\det A}\displaystyle\sum_{i=1}^n A_{ki}(-1)^{i+k}\det \tilde{A_{ki}}\\
    &= \frac{1}{\det A} \det A = 1
\end{align*}
Thus, \(AG\) has a diagonal of 1s.\\
If \(K \neq L\) then,\\
\begin{align*}
    \left(AG\right)_{KL} &= \displaystyle\sum_{i=1}^n A_{Ki}G_{iL}\\
    &= \frac{1}{\det A} \displaystyle\sum_{i=1}^n A_{Ki}(-1)^{i+L} \det \tilde{A_{Li}}
\end{align*}
The sum is the determinant of the matrix
\[\begin{pmatrix}
    A_{11} & \cdots & A_{1N}\\
    \vdots & & \vdots\\
    A_{K1} & \cdots & A_{KN}\\
    \vdots & \ddots & \vdots\\
    A_{K1} & \cdots & A_{KN}\\
    \vdots & \ddots & \vdots\\
    A_{N1} & \cdots & A_{NN}
\end{pmatrix}\]
However, this matrix has two identical rows so its determinant is equal to 0.\\
This means that \(AG\) has zeroes off the diagonal.\\
\[AG = GA = I_n\]
Thus, \(A\) is invertible.
\end{prf}

\end{document}


